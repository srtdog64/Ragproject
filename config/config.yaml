chunker:
  default_params:
    language: ko
    maxTokens: 512
    overlap: 200
    paragraphMinLen: 50
    semanticThreshold: 0.82
    sentenceMinLen: 10
    windowSize: 1200
  default_strategy: adaptive
cors:
  allow_credentials: true
  allow_headers:
  - '*'
  allow_methods:
  - '*'
  allow_origins:
  - http://localhost:*
  - http://127.0.0.1:*
embedder:
  cache_dir: null
  dimension: 384
  model: multilingual-small
  type: semantic
ingester:
  batch_size: 100
  max_parallel: 8
llm:
  available_models:
    claude:
    - claude-3-5-sonnet
    - claude-3-opus
    - claude-3-sonnet
    - claude-3-haiku
    gemini:
    - gemini-2.5-flash
    - gemini-2.5-pro
    - gemini-2.0-flash
    - gemini-2.0-pro
    - gemini-1.5-flash
    - gemini-1.5-pro
    openai:
    - gpt-4o
    - gpt-4o-mini
    - gpt-4-turbo
    - gpt-4
    - gpt-3.5-turbo
  max_tokens: 2048
  model: gemini-2.5-flash
  temperature: 0.9
  type: gemini
logging:
  file: null
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  level: INFO
pipeline:
  context_compression:
    enabled: true
  generation:
    enabled: true
  parsing:
    enabled: true
    format: markdown-qa
  prompt:
    system_hint: You are a helpful RAG assistant. Answer based on the context provided.
    system_message: Be precise and helpful.
  query_expansion:
    enabled: true
    expansions: 0
  reranking:
    enabled: true
    topK: null
  retrieval:
    enabled: true
    topK: null
policy:
  defaultTopK: 10
  maxContextChars: 12000
reranker:
  boost_recent: true
  boost_title_match: true
  device: cpu
  model: cross-encoder/ms-marco-MiniLM-L-6-v2
  type: simple
server:
  host: 0.0.0.0
  log_level: info
  port: 7001
  reload: true
store:
  collection_name: rag_documents
  persist_directory: ./chroma_db
  type: chroma
