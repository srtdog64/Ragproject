# =================== RAG Configuration ===================

# === Server Settings ===
server:
  host: 0.0.0.0
  port: 7001
  reload: true
  log_level: info

# === Database Settings ===
store:
  type: chroma                          # Vector store type (chroma, faiss, memory)
  persist_directory: ./chroma_db        # Relative path to DB directory (will be relative to project root)
  collection_name: rag_documents        # Collection/namespace name
  
# === Document Watching ===  
documents:
  watched_folders:
  - C:/Users/Pc/Desktop/스크립트 백업

# === Chunking Strategy ===
chunker:
  default_strategy: adaptive             # adaptive, fixed, semantic, sentence, paragraph
  default_params:
    maxTokens: 512
    windowSize: 1200
    overlap: 200
    semanticThreshold: 0.82
    language: ko
    sentenceMinLen: 10
    paragraphMinLen: 49

# === Embedding Model ===
embedder:
  type: semantic
  model: multilingual-small
  dimension: 384
  cache_dir: null

# === Language Model ===
llm:
  type: gemini                          # gemini, openai, claude
  model: gemini-2.5-flash
  temperature: 0.01
  max_tokens: 8192
  available_models:
    gemini:
    - gemini-2.5-flash
    - gemini-2.5-pro
    - gemini-2.0-flash
    - gemini-2.0-pro
    - gemini-1.5-flash
    - gemini-1.5-pro
    openai:
    - gpt-4o
    - gpt-4o-mini
    - gpt-4-turbo
    - gpt-4
    - gpt-3.5-turbo
    claude:
    - claude-3-5-sonnet
    - claude-3-opus
    - claude-3-sonnet
    - claude-3-haiku

# === Reranking ===
reranker:
  type: bm25                            # bm25, cross-encoder, simple
  model: cross-encoder/ms-marco-MiniLM-L-6-v2
  device: cpu
  k1: 1.2
  b: 0.75
  boost_recent: true
  boost_title_match: true

# === RAG Pipeline ===
pipeline:
  query_expansion:
    enabled: true
    expansions: 0
  retrieval:
    enabled: true
    context_chunk: null
  reranking:
    enabled: true
    context_chunk: null
  context_compression:
    enabled: true
  prompt:
    system_hint: You are a helpful RAG assistant. Answer based on the context provided.
    system_message: Be precise and helpful.
  generation:
    enabled: true
  parsing:
    enabled: true
    format: markdown-qa

# === Policy Settings ===
policy:
  defaultcontext_chunk: 10
  maxContextChars: 12000

# === Ingestion ===
ingester:
  max_parallel: 8
  batch_size: 100

# === CORS Settings ===
cors:
  allow_origins:
  - http://localhost:*
  - http://127.0.0.1:*
  allow_credentials: true
  allow_methods:
  - '*'
  allow_headers:
  - '*'

# === Logging ===
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: null
